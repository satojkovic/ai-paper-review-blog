[{"figure_path": "https://arxiv.org/html/2411.07231/x1.png", "caption": "Figure 1: \nOverview.\n(a) The embedder creates an imperceptible image modification.\n(b) Traditional transformations (cropping, JPEG compression, etc.) and/or advanced manipulations (mixing watermarked and non-watermarked images, inpainting, etc.) may be applied to the image.\n(c) The extraction creates a segmentation map of watermarked parts and retrieves one or several messages.", "description": "This figure provides a visual overview of the Watermark Anything Model (WAM) workflow, demonstrating its three key stages.  Panel (a) shows the embedder's role in subtly modifying the input image to imperceptibly insert a watermark containing one or more messages. Panel (b) illustrates the robustness of the method by showing how various transformations, including standard image manipulations (cropping, compression) and more advanced techniques like splicing watermarked and non-watermarked regions or inpainting, may be applied to the watermarked image. Panel (c) showcases the extraction process, where the model segments the image, isolating watermarked areas and extracting the embedded message(s) from them. This process highlights WAM's ability to localize the watermark and retrieve messages even from altered or partially obscured images.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2411.07231/x2.png", "caption": "Figure 2: \nThe first training phase of WAM, as described in Sec.\u00a04.2,\njointly trains the watermark embedder to predict an additive watermark and the watermark extractor to detect watermarked pixels and decode the hidden message.\nIn between, the augmenter\n1) splices the watermarked and the original images based on a random mask and\n2) applies classical image transformations.", "description": "This figure illustrates the two-stage training process for the Watermark Anything Model (WAM). The first stage focuses on robustness, training an embedder to create imperceptible watermarks and an extractor to robustly detect and decode them, even after various augmentations. Augmentation involves creating a spliced image combining watermarked and original regions with a random mask, followed by additional image processing techniques.  This initial training prioritizes accurate watermark detection and decoding. A second training phase refines the model for imperceptibility and multiple watermarks. ", "section": "4 Watermark Anything Models"}, {"figure_path": "https://arxiv.org/html/2411.07231/x3.png", "caption": "(a) Without JND", "description": "This figure shows a comparison between watermarking with and without using the Just-Noticeable Difference (JND) map. The left image shows the original image, the middle image displays the watermarked image without JND application, and the right image shows the watermarked image with JND attenuation. The difference between the watermarked and original images is also shown, highlighting the impact of JND on imperceptibility.  The watermark is much more visible in the image without JND processing than the image that includes JND.", "section": "4.3 Post-training for imperceptibility and multiple watermarks"}, {"figure_path": "https://arxiv.org/html/2411.07231/x4.png", "caption": "(b) With JND", "description": "This figure shows the impact of using a Just-Noticeable-Difference (JND) map on the imperceptibility of a watermark. The left image shows a watermarked image where the watermark is highly perceptible (PSNR \u2248 24 dB). The right image shows a watermarked image after applying JND attenuation (PSNR \u2248 36 dB), making the watermark more imperceptibly hidden because the watermark's intensity is modulated according to human visual sensitivity, making it less visible in areas where the eye is not as sensitive to changes.", "section": "4.3 Post-training for imperceptibility and multiple watermarks"}, {"figure_path": "https://arxiv.org/html/2411.07231/x5.png", "caption": "Figure 3: \nImpact of the JND map on imperceptibility.\n(Left) After the first training phase, the watermark is highly perceptible.\n(Right) When applying the JND attenuation, it is hidden in areas where the eye is not sensitive to changes, which makes it less visible.\nFine-tuning with the JND recovers the initial robustness.\nThe difference is displayed as 10\u00d7abs\u2062(xm\u2212x)10abssubscript\ud835\udc65m\ud835\udc6510\\times\\text{abs}(x_{\\text{m}}-x)10 \u00d7 abs ( italic_x start_POSTSUBSCRIPT m end_POSTSUBSCRIPT - italic_x ).", "description": "This figure demonstrates the effect of using a Just-Noticeable-Difference (JND) map on watermark imperceptibility. The left image shows a watermark that is highly visible after the initial training phase. By incorporating JND attenuation during the second training phase, the watermark becomes less perceptible to the human eye, particularly in areas where the visual system is less sensitive to changes. Finally, fine-tuning with the JND map restores the original robustness. The difference image (10 * abs(xm-x)) visually highlights the degree of change introduced by the watermark.", "section": "4.3 Post-training for imperceptibility and multiple watermarks"}, {"figure_path": "https://arxiv.org/html/2411.07231/x6.png", "caption": "Figure 4: Evaluation of the localization on the validation set of COCO, with or without cropping before extraction, following the setup described in Sec.\u00a05.4.\n(Left) Localization accuracy using intersection over union between the predicted watermarked areas and the ground-truth mask.\n(Right) Bit accuracy between the ground truth message and the decoded message, computed from Eq.\u00a0(2).", "description": "Figure 4 presents a comparative analysis of watermark localization accuracy, using the Intersection over Union (IoU) metric, and decoding accuracy (bit accuracy), measured against ground truth, on the COCO validation dataset.  The experiment involved scenarios with and without prior image cropping, as detailed in Section 5.4. The left panel displays IoU values illustrating the spatial overlap between predicted and actual watermarked regions, while the right panel shows the bit accuracy of the decoded messages, calculated using Equation 2 from the paper. This figure demonstrates the robustness of the method across different image manipulation scenarios.", "section": "Experiments & Results"}, {"figure_path": "https://arxiv.org/html/2411.07231/x7.png", "caption": "(a) After first training phase (PSNR \u224825absent25\\approx 25\u2248 25\u00a0dB)", "description": "This figure shows the results of the watermarking process after the first training phase.  The watermark is clearly visible, indicated by a PSNR (Peak Signal-to-Noise Ratio) of approximately 25 dB.  This low PSNR value reflects a significant difference between the original and watermarked images.  The image illustrates the lack of imperceptibility at this stage of training.", "section": "4 Watermark Anything Models"}, {"figure_path": "https://arxiv.org/html/2411.07231/x12.png", "caption": "(b) After second training phase (PSNR \u224838absent38\\approx 38\u2248 38\u00a0dB)", "description": "This figure shows the impact of the second training phase on the imperceptibility of the watermark.  The left image shows the original image, and the center image shows the result after the first training phase, where the watermark is quite visible. The right image displays the result of the second training phase, where the watermark is significantly less visible due to incorporating a Just-Noticeable Difference (JND) map. The improved imperceptibility is quantified by a peak signal-to-noise ratio (PSNR) of approximately 38 dB.", "section": "4.3 Post-training for imperceptibility and multiple watermarks"}, {"figure_path": "https://arxiv.org/html/2411.07231/x13.png", "caption": "Figure 5: \nResults on multiple watermarks extracted from a single image.\nWe use non overlapping 10\u00a0% rectangular masks to watermark up to 5 parts of images from COCO, with different messages, and report the average number of clusters detected by DBSCAN, bit accuracy across found messages, as well as the mIoU of watermark detection on all objects.\n(Left) After the first training phase, the bit accuracy strongly decreases as the number of watermarks grows. (Right) After fine-tuning, it stays roughly constant no matter the number of watermarked parts.\nThe mIoU stays stable in both cases.", "description": "This figure shows the results of extracting multiple watermarks from a single image.  The experiment uses non-overlapping rectangular masks (10% of the image area each) to embed up to five different messages in separate parts of COCO images. The figure displays the average number of clusters identified by the DBSCAN algorithm, the overall bit accuracy of the recovered messages, and the mean Intersection over Union (mIoU) score for watermark detection. The left panel shows the results after the first training phase, demonstrating that bit accuracy decreases significantly as the number of watermarks increases. The right panel displays the results after a fine-tuning stage, illustrating that bit accuracy remains stable even when multiple watermarks are present.  The mIoU for watermark detection also remains stable across both scenarios.", "section": "5.5 Multiple watermarks"}, {"figure_path": "https://arxiv.org/html/2411.07231/x14.png", "caption": "(a) Random masks", "description": "This figure shows examples of randomly generated masks used in the training process of the Watermark Anything Model (WAM).  The masks are used to randomly occlude or reveal portions of watermarked images, thus enhancing the model's robustness against various image manipulations. Different types of masks are shown, including full-image masks covering the entire image, box-shaped masks with randomly determined sizes, and irregular, brush-stroke-like masks.", "section": "4.2 Pre-training models for localized message embedding and extraction"}, {"figure_path": "https://arxiv.org/html/2411.07231/x15.png", "caption": "(b) Segmentation masks", "description": "This figure shows examples of segmentation masks used in the training process. These masks represent areas of the image that are intended to be watermarked.  Different types of masks are included, such as full masks that cover the entire image, box-shaped masks that are rectangular regions, irregular masks that are free-form shapes, and masks based on existing object segmentations from the COCO dataset. The variety in mask shapes and sizes ensures that the model learns to robustly detect and extract watermarks regardless of the watermarked area's characteristics.", "section": "4.2 Pre-training models for localized message embedding and extraction"}, {"figure_path": "https://arxiv.org/html/2411.07231/x16.png", "caption": "Figure 6: \nExamples of masks used during training.\nOnly the white areas of the image end up being watermarked.\n(a) Random masks (irregular, rectangles, inverted, full, null or inverted).\n(b) Segmentation masks created from the union of COCO\u2019s segmentation masks.", "description": "Figure 6 shows examples of the masks used to train the Watermark Anything Model (WAM). The masks determine which parts of an image will be watermarked.  Panel (a) displays various random masks, including irregular shapes, rectangles, inverted versions, and completely filled or empty masks. Panel (b) shows segmentation masks generated from the COCO dataset, where the masks define areas corresponding to object boundaries.", "section": "4 Watermark Anything Models"}, {"figure_path": "https://arxiv.org/html/2411.07231/x17.png", "caption": "Figure 7: \nExperimental protocol for the evaluation of watermark localization as performed in Sec.\u00a05.4.", "description": "This figure illustrates the experimental setup used to evaluate the watermark localization capabilities of the Watermark Anything Model (WAM).  The process begins with an original image. A 60% watermarked area is added to the original image. This watermarked image is then modified by cropping the upper-left 25% of the image and resizing it back to the original dimensions. The resulting image is then input to the WAM extractor to evaluate its ability to accurately locate the watermarked region, even after these transformations.", "section": "5.4 Localization"}, {"figure_path": "https://arxiv.org/html/2411.07231/x18.png", "caption": "(a) After the first phase of the training, evaluation without augmentation", "description": "This figure shows the results of watermark detection and decoding after the first training phase of the Watermark Anything Model (WAM).  The first training phase focuses on robustness, not imperceptibility, so the watermark is very visible. The images show the original image, the watermarked image, a mask indicating the watermarked regions, the extracted detection mask, and the decoded message.  The results demonstrate the model's ability to accurately locate and decode watermarks at this training stage, even without further augmentations. Note that this is before the model has been fine-tuned for visual invisibility.", "section": "4 Watermark Anything Models"}]