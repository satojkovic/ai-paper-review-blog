[{"content": "| Method |  | HiDDeN | DCTDWT | SSL | FNNS | TrustMark | WAM (ours) |\n|---|---|---|---|---|---|---|---|---|\n| COCO | PSNR (\u2191) | 38.2 | 37.0 | 37.8 | 37.7 | 40.3 | 38.3 |\n|  | SSIM (\u2191) | 0.98 | 0.98 | 0.98 | 0.98 | 0.99 | 0.99 |\n|  | LPIPS (\u2193) | 0.05 | 0.02 | 0.07 | 0.06 | 0.01 | 0.04 |\n| DIV2K | PSNR (\u2191) | 38.4 | 38.7 | 38.2 | 39.0 | 39.1 | 38.8 |\n|  | SSIM (\u2191) | 0.98 | 0.99 | 0.98 | 0.99 | 0.99 | 0.99 |\n|  | LPIPS (\u2193) | 0.07 | 0.03 | 0.11 | 0.04 | 0.01 | 0.03 |", "caption": "Table 1: \nEvaluation of the watermark imperceptibility.\nWe report the PSNR, SSIM, and LPIPS between watermarked and original images of COCO (low/mid-resolution) and DIV2k (high-resolution).", "description": "This table presents a quantitative evaluation of the watermark's imperceptibility.  It compares the watermarked and original images using three metrics: Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS).  The results are shown for images from two datasets: COCO (at lower and medium resolutions) and DIV2k (at high resolution), providing a comprehensive assessment of the watermark's impact on image quality across different resolutions and image types.", "section": "5.2 Quality"}, {"content": "| Method | FPR | TPR | Bit acc. | TPR | Bit acc. | TPR | Bit acc. | TPR | Bit acc. | TPR | Bit acc. |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| HiDDeN | 0.08 | 76.9 | 95.5 | 31.2 | 80.1 | 48.4 | 87.2 | 44.7 | 88.7 | 0.9 | 68.0 |\n| DWTDCT | 0.02 | 77.1 | 91.4 | 0.0 | 50.5 | 13.6 | 58.1 | 47.8 | 81.7 | 0.8 | 59.9 |\n| SSL | 0.00 | 99.9 | 100.0 | 14.3 | 76.5 | 70.5 | 92.1 | 67.7 | 91.1 | 0.4 | 58.9 |\n| FNNS | 0.10 | 99.6 | 99.9 | 62.4 | 86.6 | 82.1 | 93.9 | 89.4 | 97.3 | 38.7 | 88.5 |\n| TrustMark | 2.88 | 99.8 | 99.9 | 36.3 | 71.4 | 90.1 | 98.2 | 39.4 | 83.2 | 83.2 | 57.1 |\n| WAM (ours) | 0.04 | 100.0 | 100.0 | 99.3 | 91.8 | 100.0 | 99.9 | 97.9 | 99.2 | 100.0 | 95.3 |", "caption": "Table 2: \nDetection and decoding after image editing (detailed in Sec.\u00a010.4).\nWe show the bit accuracy (Bit acc.) between the encoded and decoded messages, the proportion of images correctly deemed watermarked (TPR), and the proportion of non-watermarked images falsely detected as watermarked (FPR), in %.\nSince HiDDeN, DCTDWT, SSL and FNNS do not naturally provide a detection result, we hide 48 bits and reserve 16 bits for detection, and flag an image as watermarked if it has strictly less than two bits incorrectly decoded (these baselines are detailed in Sec.\u00a05.3).", "description": "This table presents the performance of different watermarking methods on image detection and decoding after applying various image editing techniques (geometric transformations, valuemetric adjustments, splicing, and inpainting).  It shows the bit accuracy of the decoded messages, the true positive rate (TPR) of correctly identifying watermarked images, and the false positive rate (FPR) of incorrectly classifying non-watermarked images as watermarked.  Because some methods do not have built-in detection, a modified approach is used for them: 48 bits were embedded, and 16 were used for detection.  An image was considered watermarked if fewer than 2 bits were decoded incorrectly. Results are shown separately for low/mid-resolution and high-resolution image datasets.", "section": "5.3 Detection and decoding"}, {"content": "| Method | FP | TPR | Bit acc. | TPR | Bit acc. | TPR | Bit acc. | TPR | Bit acc. | TPR | Bit acc. |\n|---|---|---|---|---|---|---|---|---|---|---|\n|  |  | \nNone | Geometric | Valuemetric | Inpainting | Splicing |\n|---|---|---|---|---|---|---|---|---|---|---|\n| HiDDeN | 0 | 72.0 | 95.8 | 33.5 | 81.3 | 53.3 | 88.1 | 53.3 | 88.1 | 1.5 | 70.2 |\n| DWTDCT | 0 | 75.0 | 88.9 | 0.0 | 50.6 | 18.4 | 58.9 | 73.0 | 86.7 | 31.5 | 71.6 |\n| SSL | 0 | 100.0 | 100.0 | 32.5 | 83.6 | 75.8 | 92.8 | 95.0 | 96.1 | 0.5 | 59.8 |\n| FNNS | 0 | 97.0 | 99.9 | 63.5 | 87.2 | 79.8 | 93.9 | 94.0 | 99.2 | 48.5 | 89.5 |\n| TrustMark | 5 | 100.0 | 100.0 | 33.1 | 70.5 | 87.4 | 97.9 | 0.0 | 72.1 | 1.5 | 57.3 |\n| WAM (ours) | 0 | 100.0 | 99.9 | 96.1 | 89.0 | 100.0 | 99.9 | 100.0 | 99.8 | 99.5 | 94.2 |", "caption": "(a) On the first 10k validation images of COCO (low to mid resolution)", "description": "This table presents the results of watermark detection and decoding experiments conducted on the first 10,000 validation images of the COCO dataset, focusing on low to mid-resolution images. It compares the performance of WAM with other state-of-the-art watermarking methods (HiDDeN, DCTDWT, SSL, FNNS, TrustMark) across various image augmentations, including geometric transformations, valuemetric adjustments, inpainting, and splicing. The metrics used for comparison include the false positive rate (FPR), true positive rate (TPR), and bit accuracy.  Each augmentation type's impact on each method's performance is shown.", "section": "5.3 Detection and decoding"}, {"content": "Encoder|Decoder\n---|---|---\n$x\\in\\mathbb{R}^{3\\times H\\times W}, m\\in\\{0,1\\}^{n_{\\text{bits}}}$|$(z, z_{\\text{msg}})\\in\\mathbb{R}^{(d_{\\text{z}}+d_{\\text{msg}})\\times 32\\times 32}$ \nInterpolation, Conv2D $\\to\\mathbb{R}^{d\\times 256\\times 256}$|Conv2D $\\to\\mathbb{R}^{d^{'}\\times 32\\times 32}$ \n$m\\times\\{\\text{Residual Block, Down Block}\\}\\to\\mathbb{R}^{d^{'}\\times 32\\times 32}$|Residual Block $\\to\\mathbb{R}^{d^{'}\\times 32\\times 32}$ \nResidual Block $\\to\\mathbb{R}^{d^{'}\\times 32\\times 32}$|Non-Local Block $\\to\\mathbb{R}^{d^{'}\\times 32\\times 32}$ \nNon-Local Block $\\to\\mathbb{R}^{d^{'}\\times 32\\times 32}$|Residual Block $\\to\\mathbb{R}^{d^{'}\\times 32\\times 32}$ \nResidual Block $\\to\\mathbb{R}^{d^{'}\\times 32\\times 32}$|$m\\times\\{\\text{Residual Block, Up Block}\\}\\to\\mathbb{R}^{d\\times 256\\times 256}$ \nGroupNorm, Swish, Conv2D $\\to\\mathbb{R}^{d_{\\text{z}}\\times 32\\times 32}$|GroupNorm, Swish, Conv2D $\\to\\mathbb{R}^{3\\times 256\\times 256}$ \n$\\mathcal{T}_{\\theta}(m), \\text{Repeat } \\to\\mathbb{R}^{d_{\\text{msg}}\\times 32\\times 32}$|TanH, Interpolation $\\to[-1,1]^{3\\times H\\times W}$", "caption": "(b) On the 100 validation images of DIV2k (high resolution)", "description": "This table presents the detection and decoding results for high-resolution images from the DIV2k dataset.  It shows the bit accuracy (how many bits of the embedded message were correctly decoded), the true positive rate (TPR, the percentage of watermarked images correctly identified as such), and the false positive rate (FPR, the percentage of non-watermarked images incorrectly identified as watermarked).  The results are broken down by different image augmentations (transformations applied to the image such as geometric or valuemetric changes).  The augmentations include 'none' (no transformation), geometric transformations (like rotations, resizing, etc.), valuemetric transformations (brightness, contrast, filtering), inpainting (filling in missing parts of the image), and splicing (combining parts of different images).  This allows for the assessment of the model's robustness to various types of image manipulations.", "section": "5.3 Detection and decoding"}, {"content": "| Image encoder (ViT) | Pixel decoder (CNN) |\n|---|---| \n| <math alttext=\"x\\in\\mathbb{R}^{3\\times H\\times W}\" display=\"inline\">x \u2208 \u211d<sup>3 \u00d7 H \u00d7 W</sup></math> | <math alttext=\"z\\in\\mathbb{R}^{d' \\times 16\\times 16}\" display=\"inline\">z \u2208 \u211d<sup>d' \u00d7 16 \u00d7 16</sup></math> |\n| Interpolation <math alttext=\"\\to\\mathbb{R}^{3\\times 256\\times 256}\" display=\"inline\">\u2192 \u211d<sup>3 \u00d7 256 \u00d7 256</sup></math> | <math display=\"inline\">m' \u00d7 \\{Residual Block, Up Block\\} \\to \\mathbb{R}^{d''\\times 256\\times 256}</math> |\n| Patch Embed (Conv2D), Pos. Embed <math alttext=\"\\to\\mathbb{R}^{d\\times 16\\times 16}\" display=\"inline\">\u2192 \u211d<sup>d \u00d7 16 \u00d7 16</sup></math> | Linear <math alttext=\"\\to\\mathbb{R}^{(1+n_{\\text{bits}})\\times 256\\times 256}\" display=\"inline\">\u2192 \u211d<sup>(1+n<sub>bits</sub>) \u00d7 256 \u00d7 256</sup></math> |\n| <math display=\"inline\">m \\times \\{Transformer Block\\} \\to \\mathbb{R}^{d\\times 16\\times 16}</math> | Sigmoid (optional) <math alttext=\"\\to\\mathbb{R}^{(1+n_{\\text{bits}})\\times 256\\times 256}\" display=\"inline\">\u2192 \u211d<sup>(1+n<sub>bits</sub>) \u00d7 256 \u00d7 256</sup></math> |\n| LayerNorm, GELU, Conv2D <math alttext=\"\\to\\mathbb{R}^{d'\\times 16\\times 16}\" display=\"inline\">\u2192 \u211d<sup>d' \u00d7 16 \u00d7 16</sup></math> | Interpolation <math alttext=\"\\to\\mathbb{R}^{(1+n_{\\text{bits}})\\times H\\times W}\" display=\"inline\">\u2192 \u211d<sup>(1+n<sub>bits</sub>) \u00d7 H \u00d7 W</sup></math> |", "caption": "Table 3: High-level architecture of the encoder and decoder of the watermark embedder emb\u03b8subscriptemb\ud835\udf03\\mathrm{emb}_{\\theta}roman_emb start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT.\nThe design of the networks follows the architecture presented by Ho et\u00a0al. (2020); Esser et\u00a0al. (2021); Rombach et\u00a0al. (2022).", "description": "Table 3 details the architecture of the encoder and decoder components within the watermark embedder of the Watermark Anything Model (WAM).  It shows the layers, their types (e.g., Convolutional, Residual Block, Non-Local Block), and the dimensions of their outputs. The design is based on previous work by Ho et al. (2020), Esser et al. (2021), and Rombach et al. (2022), indicating a convolutional autoencoder structure.", "section": "4 Watermark Anything Models"}, {"content": "| Identity | Contrast 0.5 | Contrast 1.5 | Brightness 0.5 | Brightness 1.5 |\n|---|---|---|---|---|\n| ![Identity](https://arxiv.org/html/2411.07231/figs/appendix/augs/Identity.jpeg) | ![Contrast_strength_0.5](https://arxiv.org/html/2411.07231/figs/appendix/augs/Contrast_strength_0.5.jpeg) | ![Contrast_strength_1.5](https://arxiv.org/html/2411.07231/figs/appendix/augs/Contrast_strength_1.5.jpeg) | ![Brightness_strength_0.5](https://arxiv.org/html/2411.07231/figs/appendix/augs/Brightness_strength_0.5.jpeg) | ![Brightness_strength_1.5](https://arxiv.org/html/2411.07231/figs/appendix/augs/Brightness_strength_1.5.jpeg) |\n| Hue 0.5 | Saturation 1.5 | Median filter 7 | Gaussian blur 17 | JPEG 40 |\n|---|---|---|---|---|\n| ![Hue_strength_-0.5](https://arxiv.org/html/2411.07231/figs/appendix/augs/Hue_strength_-0.5.jpeg) | ![Saturation_strength_1.5](https://arxiv.org/html/2411.07231/figs/appendix/augs/Saturation_strength_1.5.jpeg) | ![MedianFilter_strength_7](https://arxiv.org/html/2411.07231/figs/appendix/augs/MedianFilter_strength_7.jpeg) | ![GaussianBlur_strength_17](https://arxiv.org/html/2411.07231/figs/appendix/augs/GaussianBlur_strength_17.jpeg) | ![JPEG_strength_40](https://arxiv.org/html/2411.07231/figs/appendix/augs/JPEG_strength_40.jpeg) |\n| Crop 0.33 | Resize 0.5 | Rotation 10 | Perspective 0.5 | Horizontal flipping |\n|---|---|---|---|---|\n| ![Crop_strength_0.33](https://arxiv.org/html/2411.07231/figs/appendix/augs/Crop_strength_0.33.jpeg) | ![Resize_strength_0.5](https://arxiv.org/html/2411.07231/figs/appendix/augs/Resize_strength_0.5.jpeg) | ![Rotate_strength_10](https://arxiv.org/html/2411.07231/figs/appendix/augs/Rotate_strength_10.jpeg) | ![Perspective_strength_0.5](https://arxiv.org/html/2411.07231/figs/appendix/augs/Perspective_strength_0.5.jpeg) | ![HorizontalFlip](https://arxiv.org/html/2411.07231/figs/appendix/augs/HorizontalFlip.jpeg) |", "caption": "Table 4: High-level architecture of the encoder and decoder of the watermark extractor ext\u03b8subscriptext\ud835\udf03\\mathrm{ext}_{\\theta}roman_ext start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT.\nThe design of the networks follows the architecture presented by Zheng et\u00a0al. (2021); Kirillov et\u00a0al. (2023).", "description": "Table 4 details the architecture of the watermark extractor, a key component of the Watermark Anything Model (WAM).  It outlines the structure of both the encoder (using a Vision Transformer or ViT) and the decoder (a Convolutional Neural Network or CNN). The design is inspired by previous works on semantic segmentation and image transformers, specifically referencing Zheng et al. (2021) and Kirillov et al. (2023). The table provides a layer-by-layer breakdown of the network, including input/output dimensions, activation functions, and other relevant details.", "section": "4 Watermark Anything Models"}, {"content": "| Method | AUC: Stable Sig. | AUC: Tree-Ring | AUC: WAM | TPR@1e-2: Stable Sig. | TPR@1e-2: Tree-Ring | TPR@1e-2: WAM | TPR@1e-4: Stable Sig. | TPR@1e-4: Tree-Ring | TPR@1e-4: WAM |\n|---|---|---|---|---|---|---|---|---|---| \n| None | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 |\n| Valuemetric | 0.94 | 1.00 | 1.00 | 0.90 | 1.00 | 1.00 | 0.90 | 1.00 | 1.00 |\n| Geometric | 1.00 | 0.91 | 1.00 | 0.97 | 0.56 | 1.00 | 0.87 | 0.43 | 1.00 |\n| Comb. | 1.00 | 0.75 | 1.00 | 0.99 | 0.05 | 1.00 | 0.99 | 0.01 | 1.00 |\n| Splicing | 0.97 | 0.72 | 1.00 | 0.90 | 0.00 | 1.00 | 0.81 | 0.00 | 0.00 |", "caption": "Table 5: Illustration of transformations evaluated in Sec.\u00a05.", "description": "This table showcases examples of various image transformations used in Section 5 of the paper to evaluate the robustness of the watermarking model.  The transformations are categorized into valuemetric (adjusting pixel values), geometric (modifying image geometry), and others. Each column represents a different transformation type with specific parameters used during evaluation. The images visually illustrate the effects of these transformations.", "section": "5 Experiments & Results"}, {"content": "HiDDeN|DCTDWT|SSL|FNNS|TrustMark|WAM\n---|---|---|---|---|---|---\nTPR / Bit acc.|TPR / Bit acc.|TPR / Bit acc.|TPR / Bit acc.|TPR / Bit acc.|TPR / Bit acc.\nNone|76.0|95.5|77.1|91.4|99.9|100.0|99.6|99.9|99.8|99.9|100.0\nCrop (20%)|66.5|93.3|0.1|50.2|3.3|68.8|98.5|99.6|2.1|50.3|99.7|88.8\nCrop (50%)|71.0|94.6|0.0|50.4|7.0|73.4|99.2|99.8|1.9|60.9|99.6|95.9\nRot (10%)|7.1|80.8|0.0|50.7|11.2|78.1|76.0|94.9|3.0|56.4|97.7|77.0\nPerspective (0.1)|65.7|93.6|0.0|51.8|15.6|80.0|98.0|99.6|77.8|96.7|100.0|99.8\nPerspective (0.5)|32.3|89.1|0.1|50.0|0.8|61.3|51.6|91.3|2.6|50.8|100.0|96.0\nHorizontal Flip|0.1|54.3|0.0|50.2|32.7|86.2|0.1|56.2|99.8|99.9|100.0|100.0\nBrightness (1.5)|85.6|96.9|16.1|60.9|90.9|97.7|99.1|99.7|83.2|97.1|100.0|100.0\nBrightness (2.0)|83.2|96.2|0.1|49.4|67.8|91.9|97.2|99.0|58.8|92.2|100.0|99.9\nContrast (1.5)|74.1|95.4|20.4|63.4|92.9|98.2|98.9|99.7|77.1|96.2|100.0|100.0\nContrast (2.0)|67.3|94.3|0.6|49.4|66.0|92.4|97.0|99.4|52.1|90.8|100.0|99.9\nHue (-0.1)|9.0|77.4|29.5|66.7|98.0|99.4|96.7|99.2|99.3|99.9|100.0|100.0\nHue (+0.1)|19.3|84.0|59.4|81.2|97.7|99.3|98.5|99.5|99.5|99.9|100.0|100.0\nSaturation (1.5)|80.6|96.2|21.3|61.7|99.7|99.9|99.5|99.9|99.0|99.8|100.0|100.0\nSaturation (2.0)|81.7|96.5|0.3|47.4|98.3|99.5|99.4|99.8|96.8|99.5|100.0|100.0\nMedian filter (3)|74.6|94.9|0.9|53.2|82.8|96.5|99.4|99.9|99.7|99.9|100.0|100.0\nMedian filter (7)|23.4|83.0|0.4|53.0|20.4|80.7|61.1|90.2|99.4|99.9|100.0|100.0\nGaussian Blur (3)|47.1|88.0|41.7|77.0|97.2|99.1|87.3|95.5|99.8|99.9|100.0|100.0\nGaussian Blur (17)|0.1|51.2|0.0|49.8|3.8|69.6|0.0|50.5|98.7|99.8|100.0|99.8\nJPEG (50)|11.2|77.3|0.0|49.8|5.1|72.5|34.5|86.9|99.1|99.8|99.9|99.0\nJPEG (80)|32.8|86.8|0.1|50.5|66.1|92.6|80.5|95.4|99.6|99.9|100.0|99.9\nProportion (10%)|0.8|65.9|1.1|56.9|0.6|61.8|36.3|88.2|2.0|58.6|99.9|94.2\nCollage (10%)|0.9|70.1|0.5|62.8|0.1|55.9|41.1|88.7|1.3|55.7|100.0|96.5\nCombination|44.7|88.7|0.0|49.9|1.2|62.8|87.7|96.1|1.7|58.8|99.2|87.2", "caption": "Table 6: \nDetection results for WAM and generation-time watermarking methods, on 1k negative (non-watermarked), and 1k positive (watermarked), possibly edited, images.\nAUC refers to the Area Under the ROC curve, TPR@10\u22122superscript10210^{-2}10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT is the TPR at FPR=10\u22122absentsuperscript102=10^{-2}= 10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT.\nStable Signature\u00a0(Fernandez et\u00a0al., 2023a) embeds a 48-bit message and uses the bit accuracy as score, while Tree-Ring\u00a0(Wen et\u00a0al., 2023) and WAM output a detection score.\nWAM is competitive with watermarking methods for generative models (although the latter offer noteworthy advantages).", "description": "Table 6 presents a comparison of watermark detection performance between WAM and other generation-time watermarking methods.  The methods were tested on 1000 non-watermarked and 1000 watermarked images (which may have been altered).  The table shows the Area Under the Curve (AUC) of the ROC curve, True Positive Rate (TPR) when the False Positive Rate (FPR) is 10\u207b\u00b2, and TPR when the FPR is 10\u207b\u2074.  Note that Stable Signature uses bit accuracy as its score, whereas Tree-Ring and WAM use a detection score.  Overall, WAM shows competitive performance compared to existing methods, although the generation-time methods offer some advantages.", "section": "5.3 Detection and decoding"}, {"content": "| Original | Watermarked | Difference | Original | Watermarked | Difference |\n|---|---|---|---|---|---|---|\n| ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/10_nw.jpg) | ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/10_w.jpg) | ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/10_diff.jpg) | ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/01_nw.jpg) | ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/01_w.jpg) | ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/01_diff.jpg) |\n| ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/02_nw.jpg) | ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/02_w.jpg) | ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/02_diff.jpg) | ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/03_nw.jpg) | ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/03_w.jpg) | ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/03_diff.jpg) |\n| ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/04_nw.jpg) | ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/04_w.jpg) | ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/04_diff.jpg) | ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/05_nw.jpg) | ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/05_w.jpg) | ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/05_diff.jpg) |\n| ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/00_nw.jpg) | ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/00_w.jpg) | ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/00_diff.jpg) | ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/11_nw.jpg) | ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/11_w.jpg) | ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/11_diff.jpg) |\n| ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/12_nw.jpg) | ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/12_w.jpg) | ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/12_diff.jpg) | ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/13_nw.jpg) | ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/13_w.jpg) | ![Refer to caption](https://arxiv.org/html/2411.07231/figs/appendix/coco/13_diff.jpg) |", "caption": "Table 7: \nFull decoding results on the COCO validation set, used for the aggregated results presented in Tab.\u00a02(a) of Sec\u00a05.3.\n\u201cCombination\u201d corresponds to JPEG-80, Brightness 1.5 and Crop 50% all on the same image, and was not part of the evaluation of Sec.\u00a05.3.\nTPR is for a threshold on the detection score sdetsubscript\ud835\udc60dets_{\\text{det}}italic_s start_POSTSUBSCRIPT det end_POSTSUBSCRIPT such that FPR =0.04%absentpercent0.04=0.04\\%= 0.04 %.\nThe bit accuracy is computed as described in Eq.\u00a02.", "description": "Table 7 presents a detailed breakdown of the watermarking model's performance on the COCO validation dataset, focusing on detection and decoding accuracy.  It expands upon the aggregated results shown in Table 2(a) of Section 5.3, providing more granular insights into the model's robustness against various image manipulations. Each row represents a different type of image transformation (e.g., cropping, brightness adjustment, JPEG compression, etc.), and the columns show the True Positive Rate (TPR), False Positive Rate (FPR), and bit accuracy.  A combination of multiple transformations is also tested.  The TPR is calculated using a threshold on the detection score that maintains an FPR of 0.04%. Bit accuracy reflects the percentage of correctly decoded bits in the embedded message.", "section": "5.3 Detection and decoding"}]